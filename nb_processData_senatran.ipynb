{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Setup Inicial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Bibliotecas utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Definições gerais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define os diretórios\n",
        "\n",
        "\n",
        "blob_container_name = 'general' # replace with your container name\n",
        "blob_relative_path_raw = 'raw/mercado_potencial/senatran/' # replace with your relative folder path\n",
        "blob_relative_path_enriched = 'enriched/mercado_potencial/senatran/' # replace with your relative folder path\n",
        "linked_service_raw = 'LS_ADLS_RAW_01' # replace with your linked service name\n",
        "linked_service_enriched = 'LS_ADLS_ENRICHED_01' # replace with your linked service name\n",
        "\n",
        "\n",
        "ls_raw = mssparkutils.credentials.getPropertiesAll(linked_service_raw)\n",
        "ls_enriched = mssparkutils.credentials.getPropertiesAll(linked_service_enriched)\n",
        "\n",
        "converter_dic_raw = json.loads(ls_raw)\n",
        "converter_dic_enriched = json.loads(ls_enriched)\n",
        "\n",
        "#coletando o endpoint\n",
        "end_point_raw = (converter_dic_raw['Endpoint'].split(\"/\"))[2]\n",
        "end_point_enriched = (converter_dic_enriched['Endpoint'].split(\"/\"))[2]\n",
        "\n",
        "#Utilizado na leitura via metodo mssparkutils\n",
        "abfss_path_raw = 'abfss://%s@%s/%s' % (blob_container_name, end_point_raw, blob_relative_path_raw)\n",
        "abfss_path_enriched = 'abfss://%s@%s/%s' % (blob_container_name, end_point_enriched, blob_relative_path_enriched)\n",
        "\n",
        "dir_c = abfss_path_raw + '/condutores_habilitados/'\n",
        "dir_f = abfss_path_raw + '/frota_veiculos/'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Funções de utilizadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# agrupa todos os arquivos .csv\n",
        "def agrupa_df (lista, sep=',',dec='.',enc='utf-16'):\n",
        "    \"\"\"Agrupa dos dataframes.\n",
        "\n",
        "        Lê os arquivos .csv e agrupa automaticamente os dataframes de dados do SENATRAN. \n",
        "        \n",
        "    Parâmetros\n",
        "    ----------\n",
        "    lista: lista de strings \n",
        "        Lista com os endereços dos arquivos a serem lidos\n",
        "    sep: string\n",
        "        Separador dos dados conforme função pd.read_csv.\n",
        "        padrão = ','\n",
        "    dec: string\n",
        "        simbolo de decimal conforme função pd.read_csv.\n",
        "        padrão = '.'\n",
        "    enc: string\n",
        "        encolde de dados\n",
        "        padrão = 'utf-16'\n",
        "    \n",
        "    Retorno\n",
        "    -------\n",
        "    df: pd.DataFrame\n",
        "        Dataframe com os dados agrupados.\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.DataFrame()\n",
        "    for i in lista:\n",
        "        x = pd.read_csv(i, sep=sep, decimal=dec, encoding=enc)\n",
        "        x['COMPETENCIA'] = i.split('_')[-2] +'-'+i.split('_')[-1][:-4]+'-01'\n",
        "        df = pd.concat([df,x], ignore_index=True)\n",
        "    return(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Execução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Leitura e ajuste dos dados na camada row"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cria lista de arquivos .csv\n",
        "lst_condutores = [i.path for i in mssparkutils.fs.ls(dir_c)]\n",
        "lst_frota      = [i.path for i in mssparkutils.fs.ls(dir_f)]\n",
        "\n",
        "# cria filtro para remover anos incompletos\n",
        "remove = [str(x) for x in range(2011,2018)]\n",
        "mes_dif = ['2021_12','2022_01'] # tabelas em formato diferente\n",
        "\n",
        "condutores = [i for i in lst_condutores if all(key not in i for key in remove)]\n",
        "\n",
        "# Frota tá uma salada os mes_dif estão em formato flat\n",
        "frota      = [i for i in lst_frota if all(key not in i for key in remove)]\n",
        "frota      = [i for i in lst_frota if all(key not in i for key in mes_dif)]\n",
        "frota_     = [i for i in lst_frota if any(key in i for key in mes_dif)] # meses de frota com formato diferente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_condut = agrupa_df(condutores, sep=',',dec='.',enc='utf-16')\n",
        "df_frota  = agrupa_df(frota, sep=';',dec=',',enc='utf-8')\n",
        "df_frota_  = agrupa_df(frota_, sep=';',dec=',',enc='utf-8') # dado em formato diferente dos demais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Ajustes dos dados sobre a frota de veívculos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# agrupa todas as informações de coluna tipo em unica columa \n",
        "x = df_frota.columns.str.contains('TIPO')\n",
        "df_frota['TP'] = df_frota[list(df_frota.columns[x])].bfill(axis=1).iloc[:, 0]\n",
        "# remove as anteriores\n",
        "df_frota.drop(df_frota.filter(regex='TIPO').columns, inplace=True, axis=1)\n",
        "\n",
        "# reordena as colunas\n",
        "order = [0,1,4,2,3]\n",
        "df_frota = df_frota[df_frota.columns[order].to_list()]\n",
        "\n",
        "# renomeia coluna TP para TIPO VEICULO\n",
        "df_frota.rename({'TP':'TIPO VEICULO'}, axis=1, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ajusta Frota dos dois meses em formato diferente\n",
        "x = ['UF', 'MUNICIPIO','COMPETENCIA']\n",
        "y = df_frota_.columns.difference(x).to_list()\n",
        "\n",
        "df_frota_ = pd.melt(df_frota_, id_vars=['UF','MUNICIPIO','COMPETENCIA'], value_vars=y)\n",
        "\n",
        "# reordena as colunas\n",
        "order = [0,1,3,4,2]\n",
        "df_frota_ = df_frota_[df_frota_.columns[order].to_list()]\n",
        "\n",
        "# renomeia colunas\n",
        "df_frota_.rename({'variable':'TIPO VEICULO', 'value':'QUANTIDADE'}, axis=1, inplace=True)\n",
        "\n",
        "del(x,y,order)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agrupando os dois dataframes de frota\n",
        "\n",
        "df_frota_f = pd.concat([df_frota,df_frota_], axis=0)\n",
        "df_frota_f.sort_values(['UF', 'MUNICIPIO', 'COMPETENCIA','TIPO VEICULO'], \n",
        "                      ascending=[True, True, True, True], inplace=True)\n",
        "df_frota_f.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Salvando dados mensais SENATRAN na camada eriched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "df_condut.to_parquet(abfss_path_enriched + 'senatran_condutores_habilitados.parquet', \n",
        "                     storage_options = {'linked_service':linked_service_enriched})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_frota_f.to_parquet(abfss_path_enriched + 'senatran_frota_veiculos.parquet', \n",
        "                     storage_options = {'linked_service':linked_service_enriched})"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
